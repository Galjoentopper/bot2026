{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Training Notebook - Prediction Models + PPO Agent\n",
    "\n",
    "This notebook trains both prediction models (LSTM/GRU/BiLSTM/DLSTM) and PPO trading agent with multi-horizon future predictions.\n",
    "\n",
    "**Configuration:**\n",
    "- Dataset: Configurable below (default: `ADA-EUR_1H_20240101-20251231`)\n",
    "- Task: Classification (3 classes: Fall, Stationary, Rise)\n",
    "- Sequence Length: 60\n",
    "- Models: All 4 models for ensemble\n",
    "- Prediction Horizons: [1, 2, 3, 5, 10] steps ahead (short-term + medium-term)\n",
    "\n",
    "**Usage:**\n",
    "1. Set your dataset name in Cell 1\n",
    "2. Run all cells (Runtime > Run All)\n",
    "3. All outputs saved to Google Drive automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounting Google Drive...\n",
      "============================================================\n",
      "‚ö† IMPORTANT: If using ipykernel from local IDE:\n",
      "   The authentication popup may not appear.\n",
      "   Please mount Drive manually in Colab web interface:\n",
      "   1. Open the notebook in Colab web interface\n",
      "   2. Run: from google.colab import drive; drive.mount('/content/drive')\n",
      "   3. Complete authentication in the web browser\n",
      "   4. Then return to your IDE\n",
      "============================================================\n",
      "\n",
      "üìã If you see an authentication URL below, copy it and:\n",
      "   1. Open the URL in your web browser\n",
      "   2. Sign in and authorize access\n",
      "   3. Copy the authorization code\n",
      "   4. Paste it in the input field below\n",
      "\n",
      "\n",
      "‚ö† Mount cancelled by user\n",
      "üí° You can mount Drive manually in Colab web interface\n",
      "‚ö† Error setting up paths: No module named 'colab_utils'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find 'Bot 2026' folder in Google Drive.\nPlease ensure your project is synced to Google Drive.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2449962369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mcolab_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_project_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup_environment\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'colab_utils'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2449962369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mPROJECT_PATH\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         raise FileNotFoundError(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;34m\"Could not find 'Bot 2026' folder in Google Drive.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;34m\"Please ensure your project is synced to Google Drive.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Could not find 'Bot 2026' folder in Google Drive.\nPlease ensure your project is synced to Google Drive."
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Mount Drive\n",
    "# ==============================\n",
    "\n",
    "# CONFIGURATION: Set your dataset name here\n",
    "DATASET_NAME = \"ADA-EUR_1H_20240101-20251231\"  # Change this to your dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive (handle already-mounted case)\n",
    "drive_mounted = False\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive_path = Path('/content/drive')\n",
    "    \n",
    "    # Check if Drive is already mounted\n",
    "    if drive_path.exists() and (drive_path / 'MyDrive').exists():\n",
    "        print(\"‚úì Google Drive already mounted\")\n",
    "        drive_mounted = True\n",
    "    else:\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"‚ö† IMPORTANT: If using ipykernel from local IDE:\")\n",
    "        print(\"   The authentication popup may not appear.\")\n",
    "        print(\"   Please mount Drive manually in Colab web interface:\")\n",
    "        print(\"   1. Open the notebook in Colab web interface\")\n",
    "        print(\"   2. Run: from google.colab import drive; drive.mount('/content/drive')\")\n",
    "        print(\"   3. Complete authentication in the web browser\")\n",
    "        print(\"   4. Then return to your IDE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Try to mount (will show auth URL if interactive)\n",
    "            # Note: When using ipykernel, the popup may not appear\n",
    "            # The auth URL will be printed - copy it and open in browser\n",
    "            print(\"\\nüìã If you see an authentication URL below, copy it and:\")\n",
    "            print(\"   1. Open the URL in your web browser\")\n",
    "            print(\"   2. Sign in and authorize access\")\n",
    "            print(\"   3. Copy the authorization code\")\n",
    "            print(\"   4. Paste it in the input field below\\n\")\n",
    "            \n",
    "            drive.mount('/content/drive', force_remount=False)\n",
    "            print(\"\\n‚úì Google Drive mounted successfully\")\n",
    "            drive_mounted = True\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚ö† Mount cancelled by user\")\n",
    "            print(\"üí° You can mount Drive manually in Colab web interface\")\n",
    "        except Exception as mount_error:\n",
    "            print(f\"\\n‚ö† Could not mount Drive automatically: {mount_error}\")\n",
    "            print(\"\\nüí° Solutions:\")\n",
    "            print(\"   1. Mount Drive manually in Colab web interface:\")\n",
    "            print(\"      - Open notebook in Colab (colab.research.google.com)\")\n",
    "            print(\"      - Run: from google.colab import drive; drive.mount('/content/drive')\")\n",
    "            print(\"      - Complete authentication, then return to IDE\")\n",
    "            print(\"   2. Or use the authentication URL printed above (if any)\")\n",
    "            print(\"   3. Drive mount persists for the Colab session\")\n",
    "            \n",
    "            # Check if it got mounted anyway (user might have done it manually)\n",
    "            if drive_path.exists() and (drive_path / 'MyDrive').exists():\n",
    "                print(\"\\n   ‚úì Drive appears to be mounted now!\")\n",
    "                drive_mounted = True\n",
    "except ImportError:\n",
    "    print(\"‚ö† google.colab not available - not running on Colab\")\n",
    "    print(\"  (This is OK if running locally)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Drive mount check: {e}\")\n",
    "    print(\"  (This is OK if running locally)\")\n",
    "\n",
    "# Set project paths using colab_utils\n",
    "try:\n",
    "    # Add PPO approach to path first to import colab_utils\n",
    "    # Add local Windows path first (for IDE static analysis)\n",
    "    cwd = Path.cwd()\n",
    "    local_ppo_path = cwd / 'PPO approach'\n",
    "    if str(local_ppo_path) not in sys.path:\n",
    "        sys.path.insert(0, str(local_ppo_path))\n",
    "    \n",
    "    # Also add Colab paths (for actual execution on Colab)\n",
    "    sys.path.insert(0, '/content/drive/MyDrive/Bot 2026/PPO approach')\n",
    "    sys.path.insert(0, '/content/drive/Mijn Drive/Bot 2026/PPO approach')\n",
    "    \n",
    "    from colab_utils import get_project_path, setup_environment  # type: ignore\n",
    "    \n",
    "    # Setup environment (handles Colab detection, path setup)\n",
    "    env_info = setup_environment(verbose=True)\n",
    "    PROJECT_PATH = env_info['project_path']\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error setting up paths: {e}\")\n",
    "    # Fallback paths\n",
    "    possible_paths = [\n",
    "        Path('/content/drive/MyDrive/Bot 2026'),\n",
    "        Path('/content/drive/Mijn Drive/Bot 2026'),\n",
    "    ]\n",
    "    PROJECT_PATH = None\n",
    "    for path in possible_paths:\n",
    "        if path.exists():\n",
    "            PROJECT_PATH = path\n",
    "            break\n",
    "    \n",
    "    if PROJECT_PATH is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find 'Bot 2026' folder in Google Drive.\\n\"\n",
    "            \"Please ensure your project is synced to Google Drive.\"\n",
    "        )\n",
    "\n",
    "# Verify folder exists\n",
    "if not PROJECT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Project folder not found: {PROJECT_PATH}\")\n",
    "\n",
    "# Add paths to sys.path\n",
    "if str(PROJECT_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_PATH))\n",
    "if str(PROJECT_PATH / 'PPO approach') not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_PATH / 'PPO approach'))\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "print(f\"\\n‚úì Project path: {PROJECT_PATH}\")\n",
    "print(f\"‚úì Dataset name: {DATASET_NAME}\")\n",
    "print(f\"‚úì Current directory: {os.getcwd()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "# =============================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Install TensorFlow (for prediction models)\n",
    "print(\"\\n1. Installing TensorFlow...\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"   TensorFlow already installed: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tensorflow>=2.13.0\"])\n",
    "    import tensorflow as tf\n",
    "    print(f\"   ‚úì TensorFlow installed: {tf.__version__}\")\n",
    "\n",
    "# Install PyTorch and stable-baselines3 (for PPO)\n",
    "print(\"\\n2. Installing PyTorch and stable-baselines3...\")\n",
    "try:\n",
    "    import torch\n",
    "    import stable_baselines3\n",
    "    print(f\"   PyTorch already installed: {torch.__version__}\")\n",
    "    print(f\"   stable-baselines3 already installed: {stable_baselines3.__version__}\")\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"torch\", \"stable-baselines3[extra]>=2.0.0\"])\n",
    "    import torch\n",
    "    import stable_baselines3\n",
    "    print(f\"   ‚úì PyTorch installed: {torch.__version__}\")\n",
    "    print(f\"   ‚úì stable-baselines3 installed: {stable_baselines3.__version__}\")\n",
    "\n",
    "# Install other dependencies\n",
    "print(\"\\n3. Installing other dependencies...\")\n",
    "dependencies = [\n",
    "    'pandas>=1.5.0',\n",
    "    'numpy>=1.23.0',\n",
    "    'scikit-learn>=1.2.0',\n",
    "    'matplotlib>=3.6.0',\n",
    "    'tqdm>=4.65.0',\n",
    "    'gymnasium>=0.28.0',\n",
    "    'tensorboard>=2.13.0',\n",
    "]\n",
    "\n",
    "for dep in dependencies:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep])\n",
    "        print(f\"   ‚úì {dep.split('>=')[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† {dep.split('>=')[0]}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì All dependencies installed\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Verify GPU Configuration\n",
    "# ==================================\n",
    "\n",
    "print(\"Verifying GPU Configuration...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check TensorFlow GPU availability\n",
    "print(\"\\n1. TensorFlow GPU:\")\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"   ‚úì Found {len(gpus)} GPU(s)\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"   GPU {i}: {gpu.name}\")\n",
    "        try:\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            if details:\n",
    "                print(f\"      Compute Capability: {details.get('compute_capability', 'Unknown')}\")\n",
    "        except:\n",
    "            pass\n",
    "else:\n",
    "    print(\"   ‚ö† No GPU detected for TensorFlow\")\n",
    "    print(\"   üí° In Colab: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
    "\n",
    "# Check PyTorch CUDA availability\n",
    "print(\"\\n2. PyTorch CUDA:\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚úì CUDA available: {torch.version.cuda}\")\n",
    "    print(f\"   ‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"   ‚ö† CUDA not available for PyTorch\")\n",
    "\n",
    "# Run nvidia-smi for verification\n",
    "print(\"\\n3. nvidia-smi output:\")\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader\n",
    "\n",
    "# Warning if no GPU\n",
    "if not gpus and not torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"‚ö†\"*30)\n",
    "    print(\"‚ö† WARNING: No GPU detected!\")\n",
    "    print(\"   Training will be VERY slow on CPU.\")\n",
    "    print(\"   In Google Colab: Runtime > Change runtime type > GPU\")\n",
    "    print(\"‚ö†\"*30)\n",
    "else:\n",
    "    print(\"\\n‚úì GPU configuration verified - ready for training!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Verify Dataset Exists\n",
    "# ==============================\n",
    "\n",
    "print(\"Verifying Dataset...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Get datasets path\n",
    "try:\n",
    "    from colab_utils import get_datasets_path  # type: ignore\n",
    "    datasets_path = get_datasets_path()\n",
    "except:\n",
    "    datasets_path = PROJECT_PATH / 'datasets'\n",
    "\n",
    "print(f\"Datasets directory: {datasets_path}\")\n",
    "\n",
    "# Check if dataset CSV file exists\n",
    "dataset_file = datasets_path / f\"{DATASET_NAME}.csv\"\n",
    "\n",
    "if not dataset_file.exists():\n",
    "    # Try to find with partial match\n",
    "    matches = list(datasets_path.glob(f\"*{DATASET_NAME}*.csv\"))\n",
    "    if matches:\n",
    "        dataset_file = matches[0]\n",
    "        print(f\"‚ö† Found similar file: {dataset_file.name}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå ERROR: Dataset not found: {DATASET_NAME}.csv\")\n",
    "        print(f\"\\nAvailable datasets in {datasets_path}:\")\n",
    "        for f in sorted(datasets_path.glob(\"*.csv\")):\n",
    "            print(f\"  - {f.name}\")\n",
    "        raise FileNotFoundError(\n",
    "            f\"Dataset '{DATASET_NAME}.csv' not found in {datasets_path}\\n\"\n",
    "            f\"Please update DATASET_NAME in Cell 1 or add the dataset file.\"\n",
    "        )\n",
    "\n",
    "print(f\"‚úì Dataset found: {dataset_file.name}\")\n",
    "\n",
    "# Load and display dataset info\n",
    "print(\"\\nLoading dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    \n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"  Rows: {len(df):,}\")\n",
    "    print(f\"  Columns: {len(df.columns)}\")\n",
    "    print(f\"  Columns: {', '.join(df.columns[:10])}{'...' if len(df.columns) > 10 else ''}\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\n‚ö† WARNING: Missing required columns: {missing_cols}\")\n",
    "        print(\"   Dataset may not work correctly for training\")\n",
    "    else:\n",
    "        print(f\"  ‚úì All required columns present\")\n",
    "    \n",
    "    # Display date range if timestamp exists\n",
    "    if 'timestamp' in df.columns:\n",
    "        try:\n",
    "            df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "            print(f\"\\nDate Range:\")\n",
    "            print(f\"  Start: {df['datetime'].min()}\")\n",
    "            print(f\"  End: {df['datetime'].max()}\")\n",
    "            print(f\"  Duration: {(df['datetime'].max() - df['datetime'].min()).days} days\")\n",
    "        except:\n",
    "            print(\"  (Could not parse timestamps)\")\n",
    "    \n",
    "    print(f\"\\n‚úì Dataset verified and ready for training\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train Prediction Models\n",
    "# ================================\n",
    "\n",
    "print(\"Training Prediction Models...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from configparser import ConfigParser\n",
    "\n",
    "# Import train_models functions\n",
    "sys.path.insert(0, str(PROJECT_PATH))\n",
    "from train_models import train_all_models, load_config\n",
    "\n",
    "# Load configuration\n",
    "# Priority: training_config.txt > config.txt (skip if JSON) > defaults\n",
    "config_path = PROJECT_PATH / 'training_config.txt'\n",
    "if not config_path.exists():\n",
    "    # Try config.txt, but skip if it's JSON format\n",
    "    config_path_alt = PROJECT_PATH / 'config.txt'\n",
    "    if config_path_alt.exists():\n",
    "        # Check if it's JSON (starts with {)\n",
    "        with open(config_path_alt, 'r') as f:\n",
    "            first_char = f.read(1).strip()\n",
    "        if first_char == '{':\n",
    "            print(\"‚ö† config.txt is JSON format (for crypto downloader), skipping\")\n",
    "            print(\"  Using default training configuration\")\n",
    "            config = None  # Will use defaults\n",
    "        else:\n",
    "            config_path = config_path_alt\n",
    "            print(f\"Loading config from: {config_path}\")\n",
    "            config = load_config(str(config_path))\n",
    "    else:\n",
    "        print(\"‚ö† Config file not found, using defaults\")\n",
    "        config = None\n",
    "else:\n",
    "    print(f\"Loading config from: {config_path}\")\n",
    "    config = load_config(str(config_path))\n",
    "\n",
    "# Train all models\n",
    "print(f\"\\nTraining models on dataset: {DATASET_NAME}\")\n",
    "print(\"Models: LSTM, GRU, BiLSTM, DLSTM\")\n",
    "print(\"Task: Classification (Fall, Stationary, Rise)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "try:\n",
    "    results = train_all_models(\n",
    "        datasets_dir=str(PROJECT_PATH / 'datasets'),\n",
    "        config=config,\n",
    "        task='classification',\n",
    "        models=['lstm', 'gru', 'bilstm', 'dlstm'],\n",
    "        specific_dataset=DATASET_NAME,\n",
    "        use_ensemble=False\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úì All models trained successfully!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nTraining Summary:\")\n",
    "        for result in results:\n",
    "            model_name = result['model_name']\n",
    "            metrics = result['metrics']\n",
    "            print(f\"\\n{model_name.upper()}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"  {metric}: {value:.6f}\")\n",
    "                else:\n",
    "                    print(f\"  {metric}: {value}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö† No models were trained. Check dataset name and paths.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Training completed\")\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Display Prediction Model Metrics\n",
    "# =========================================\n",
    "\n",
    "print(\"Displaying Prediction Model Metrics...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get paths\n",
    "from colab_utils import get_models_path, get_scalers_path  # type: ignore\n",
    "models_path = get_models_path()\n",
    "results_path = PROJECT_PATH / 'results'  # Prediction model results are in project root\n",
    "scalers_path = get_scalers_path()\n",
    "\n",
    "# Load and display metrics for each trained model\n",
    "models_to_check = ['lstm', 'gru', 'bilstm', 'dlstm']\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\\nModel Files:\")\n",
    "print(\"-\" * 60)\n",
    "for model_name in models_to_check:\n",
    "    model_pattern = f\"{model_name}_{DATASET_NAME}_classification.keras\"\n",
    "    model_file = models_path / model_pattern\n",
    "    \n",
    "    if not model_file.exists():\n",
    "        matches = list(models_path.glob(f\"{model_name}*{DATASET_NAME}*.keras\"))\n",
    "        if matches:\n",
    "            model_file = matches[0]\n",
    "    \n",
    "    if model_file.exists():\n",
    "        size_mb = model_file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  ‚úì {model_name.upper()}: {model_file.name} ({size_mb:.2f} MB)\")\n",
    "        trained_models[model_name] = model_file\n",
    "    else:\n",
    "        print(f\"  ‚úó {model_name.upper()}: Not found\")\n",
    "\n",
    "# Load training histories\n",
    "print(\"\\nTraining Histories:\")\n",
    "print(\"-\" * 60)\n",
    "for model_name in trained_models.keys():\n",
    "    history_file = results_path / f\"history_{model_name}_{DATASET_NAME}_classification.json\"\n",
    "    if history_file.exists():\n",
    "        with open(history_file, 'r') as f:\n",
    "            history = json.load(f)\n",
    "        \n",
    "        print(f\"\\n{model_name.upper()} Training History:\")\n",
    "        if 'accuracy' in history:\n",
    "            final_acc = history['accuracy'][-1] if history['accuracy'] else 0\n",
    "            val_acc = history['val_accuracy'][-1] if history['val_accuracy'] else 0\n",
    "            print(f\"  Final Training Accuracy: {final_acc:.4f}\")\n",
    "            print(f\"  Final Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        if 'loss' in history:\n",
    "            final_loss = history['loss'][-1] if history['loss'] else 0\n",
    "            val_loss = history['val_loss'][-1] if history['val_loss'] else 0\n",
    "            print(f\"  Final Training Loss: {final_loss:.6f}\")\n",
    "            print(f\"  Final Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "# Verify scalers\n",
    "print(\"\\nScalers:\")\n",
    "print(\"-\" * 60)\n",
    "scaler_file = scalers_path / f\"scaler_{DATASET_NAME}.pkl\"\n",
    "if scaler_file.exists():\n",
    "    print(f\"  ‚úì Scaler found: {scaler_file.name}\")\n",
    "    import pickle\n",
    "    with open(scaler_file, 'rb') as f:\n",
    "        scaler_data = pickle.load(f)\n",
    "    if isinstance(scaler_data, dict):\n",
    "        print(f\"    Sequence length: {scaler_data.get('sequence_length', 'N/A')}\")\n",
    "        print(f\"    Features: {len(scaler_data.get('feature_names', []))}\")\n",
    "else:\n",
    "    print(f\"  ‚úó Scaler not found\")\n",
    "\n",
    "# Display training plots if available\n",
    "print(\"\\nTraining Plots:\")\n",
    "print(\"-\" * 60)\n",
    "for model_name in trained_models.keys():\n",
    "    plot_file = results_path / f\"training_{model_name}_{DATASET_NAME}_classification.png\"\n",
    "    if plot_file.exists():\n",
    "        print(f\"  ‚úì {model_name.upper()} training plot: {plot_file.name}\")\n",
    "        \n",
    "        # Display the plot\n",
    "        try:\n",
    "            img = plt.imread(plot_file)\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"{model_name.upper()} Training History\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"    (Could not display: {e})\")\n",
    "    \n",
    "    cm_file = results_path / f\"confusion_{model_name}_{DATASET_NAME}.png\"\n",
    "    if cm_file.exists():\n",
    "        print(f\"  ‚úì {model_name.upper()} confusion matrix: {cm_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Metrics display completed\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Validate Models Before PPO Training\n",
    "# ============================================\n",
    "\n",
    "print(\"Validating Models Before PPO Training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Import prediction wrapper\n",
    "sys.path.insert(0, str(PROJECT_PATH / 'PPO approach'))\n",
    "from prediction_wrapper import PredictionModel, EnsemblePredictionModel\n",
    "\n",
    "# Test loading each individual model\n",
    "print(\"\\n1. Testing Individual Model Loading:\")\n",
    "print(\"-\" * 60)\n",
    "models_to_test = ['lstm', 'gru', 'bilstm', 'dlstm']\n",
    "loaded_models = {}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    try:\n",
    "        model = PredictionModel(model_name, DATASET_NAME)\n",
    "        if model.load():\n",
    "            print(f\"  ‚úì {model_name.upper()} loaded successfully\")\n",
    "            loaded_models[model_name] = model\n",
    "        else:\n",
    "            print(f\"  ‚úó {model_name.upper()} failed to load\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó {model_name.upper()} error: {e}\")\n",
    "\n",
    "# Test ensemble loading\n",
    "print(\"\\n2. Testing Ensemble Loading:\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    ensemble = EnsemblePredictionModel(DATASET_NAME, list(loaded_models.keys()))\n",
    "    if ensemble.load():\n",
    "        print(f\"  ‚úì Ensemble loaded with {len(ensemble.models)} model(s)\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Ensemble failed to load\")\n",
    "        raise RuntimeError(\"Ensemble loading failed\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚úó Ensemble error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verify model inference works\n",
    "print(\"\\n3. Testing Model Inference:\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    # Create dummy sequence for testing\n",
    "    seq_len = ensemble.sequence_length\n",
    "    # Get number of features from the first model's feature_names or scaler\n",
    "    if ensemble.models and hasattr(ensemble.models[0], 'feature_names') and ensemble.models[0].feature_names:\n",
    "        n_features = len(ensemble.models[0].feature_names)\n",
    "    elif ensemble.feature_scaler:\n",
    "        # Try to get from scaler attributes (different scikit-learn versions)\n",
    "        if hasattr(ensemble.feature_scaler, 'n_features_in_'):\n",
    "            n_features = ensemble.feature_scaler.n_features_in_\n",
    "        elif hasattr(ensemble.feature_scaler, 'feature_names_in_'):\n",
    "            n_features = len(ensemble.feature_scaler.feature_names_in_)\n",
    "        else:\n",
    "            # Fallback: get from scaler data if available\n",
    "            if hasattr(ensemble.models[0], 'scaler_data') and isinstance(ensemble.models[0].scaler_data, dict):\n",
    "                n_features = len(ensemble.models[0].scaler_data.get('feature_names', []))\n",
    "            else:\n",
    "                n_features = 27  # Default: typical number with technical indicators\n",
    "    else:\n",
    "        n_features = 27  # Default fallback\n",
    "    \n",
    "    dummy_seq = np.random.randn(seq_len, n_features)\n",
    "    print(f\"  Test sequence shape: {dummy_seq.shape}\")\n",
    "    \n",
    "    # Test single-step prediction\n",
    "    pred_class, confidence, probs = ensemble.predict(dummy_seq)\n",
    "    print(f\"  ‚úì Single-step prediction:\")\n",
    "    print(f\"    Class: {pred_class} ({['Fall', 'Stationary', 'Rise'][pred_class]})\")\n",
    "    print(f\"    Confidence: {confidence:.4f}\")\n",
    "    print(f\"    Probabilities: Fall={probs[0]:.3f}, Stationary={probs[1]:.3f}, Rise={probs[2]:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚úó Inference test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Test multi-horizon predictions\n",
    "print(\"\\n4. Testing Multi-Horizon Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    horizons = [1, 2, 3, 5, 10]\n",
    "    multi_preds = ensemble.predict_multi_horizon(dummy_seq, horizons=horizons)\n",
    "    \n",
    "    print(f\"  ‚úì Multi-horizon predictions for horizons {horizons}:\")\n",
    "    for horizon in horizons:\n",
    "        if horizon in multi_preds:\n",
    "            h_class, h_conf, h_probs = multi_preds[horizon]\n",
    "            print(f\"    t+{horizon}: {['Fall', 'Stationary', 'Rise'][h_class]} \"\n",
    "                  f\"(conf={h_conf:.3f}, probs=[{h_probs[0]:.2f}, {h_probs[1]:.2f}, {h_probs[2]:.2f}])\")\n",
    "    \n",
    "    # Test feature extraction with multi-horizon\n",
    "    print(f\"\\n  Testing feature extraction with horizons {horizons}:\")\n",
    "    features = ensemble.get_features(dummy_seq, horizons=horizons)\n",
    "    print(f\"    Feature vector shape: {features.shape}\")\n",
    "    print(f\"    Feature vector (first 10): {features[:10]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚úó Multi-horizon test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Check scaler compatibility\n",
    "print(\"\\n5. Checking Scaler Compatibility:\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    if ensemble.feature_scaler:\n",
    "        print(f\"  ‚úì Feature scaler available\")\n",
    "        # Get feature count from various possible sources\n",
    "        if ensemble.models and hasattr(ensemble.models[0], 'feature_names') and ensemble.models[0].feature_names:\n",
    "            n_features = len(ensemble.models[0].feature_names)\n",
    "            print(f\"    Features: {n_features} (from feature_names)\")\n",
    "        elif hasattr(ensemble.feature_scaler, 'n_features_in_'):\n",
    "            print(f\"    Features: {ensemble.feature_scaler.n_features_in_} (from scaler)\")\n",
    "        elif hasattr(ensemble.feature_scaler, 'feature_names_in_'):\n",
    "            print(f\"    Features: {len(ensemble.feature_scaler.feature_names_in_)} (from feature_names_in_)\")\n",
    "        else:\n",
    "            print(f\"    Features: N/A (scaler doesn't have feature count attribute)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö† Feature scaler not found\")\n",
    "    \n",
    "    print(f\"  Sequence length: {ensemble.sequence_length}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö† Scaler check warning: {e}\")\n",
    "\n",
    "# Display ensemble prediction example\n",
    "print(\"\\n6. Ensemble Prediction Example:\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    # Use a more realistic sequence (normalized)\n",
    "    test_seq = np.random.randn(seq_len, n_features) * 0.1 + 0.5  # Normalized-like values\n",
    "    \n",
    "    # Single prediction\n",
    "    pred_class, confidence, probs = ensemble.predict(test_seq)\n",
    "    print(f\"  Ensemble prediction (single-step):\")\n",
    "    print(f\"    Direction: {['Fall', 'Stationary', 'Rise'][pred_class]}\")\n",
    "    print(f\"    Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Multi-horizon\n",
    "    multi_preds = ensemble.predict_multi_horizon(test_seq, horizons=[1, 2, 3, 5, 10])\n",
    "    print(f\"  Ensemble prediction (multi-horizon):\")\n",
    "    for h in [1, 2, 3, 5, 10]:\n",
    "        if h in multi_preds:\n",
    "            h_class, h_conf, _ = multi_preds[h]\n",
    "            print(f\"    t+{h}: {['Fall', 'Stationary', 'Rise'][h_class]} (conf={h_conf:.3f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö† Example prediction warning: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Model validation completed - ready for PPO training\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train PPO Agent\n",
    "# ========================\n",
    "\n",
    "print(\"Training PPO Agent...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from configparser import ConfigParser\n",
    "\n",
    "# Change to PPO approach directory\n",
    "ppo_path = PROJECT_PATH / 'PPO approach'\n",
    "os.chdir(ppo_path)\n",
    "sys.path.insert(0, str(ppo_path))\n",
    "\n",
    "# Import PPO training functions\n",
    "from train_ppo_agent import train_ppo, load_config\n",
    "from colab_utils import get_ppo_path  # type: ignore\n",
    "\n",
    "# Load PPO configuration\n",
    "config_path = ppo_path / 'ppo_config.txt'\n",
    "print(f\"\\nLoading PPO config from: {config_path}\")\n",
    "\n",
    "if not config_path.exists():\n",
    "    print(\"‚ö† PPO config file not found, using defaults\")\n",
    "    config = load_config(None)\n",
    "else:\n",
    "    config = load_config(str(config_path))\n",
    "\n",
    "# Update dataset name in config if needed\n",
    "if config['models']['dataset'] != DATASET_NAME:\n",
    "    print(f\"\\n‚ö† Updating dataset name in config: {config['models']['dataset']} -> {DATASET_NAME}\")\n",
    "    config['models']['dataset'] = DATASET_NAME\n",
    "\n",
    "# Verify PPO config matches prediction model settings\n",
    "print(f\"\\nConfiguration Check:\")\n",
    "print(f\"  Dataset: {config['models']['dataset']}\")\n",
    "print(f\"  Sequence length: {config['environment']['sequence_length']}\")\n",
    "print(f\"  Prediction model: {config['models']['prediction_model']}\")\n",
    "\n",
    "# Configure prediction horizons\n",
    "prediction_horizons = config['models'].get('prediction_horizons', [1, 2, 3, 5, 10])\n",
    "print(f\"\\nPrediction Horizons Configuration:\")\n",
    "print(f\"  Horizons: {prediction_horizons}\")\n",
    "short_term = [h for h in prediction_horizons if h <= 3]\n",
    "medium_term = [h for h in prediction_horizons if h > 3]\n",
    "print(f\"  Short-term (1-3 steps): {short_term}\")\n",
    "print(f\"  Medium-term (5-10 steps): {medium_term}\")\n",
    "\n",
    "# Calculate observation space dimension\n",
    "# Base: 5 prediction features (t+1) + 5 price + 4 portfolio = 14\n",
    "# Additional horizons: each adds 4 features\n",
    "base_features = 5 + 5 + 4  # prediction + price + portfolio\n",
    "additional_horizons = len(prediction_horizons) - 1\n",
    "additional_features = additional_horizons * 4\n",
    "total_features = base_features + additional_features\n",
    "\n",
    "print(f\"\\nObservation Space:\")\n",
    "print(f\"  Base features: {base_features} (5 prediction + 5 price + 4 portfolio)\")\n",
    "print(f\"  Additional horizon features: {additional_features} ({additional_horizons} horizons √ó 4)\")\n",
    "print(f\"  Total observation dimension: {total_features}\")\n",
    "\n",
    "# Train PPO agent\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting PPO Training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    model = train_ppo(\n",
    "        model_type=config['models']['prediction_model'],\n",
    "        dataset=config['models']['dataset'],\n",
    "        timesteps=config['training']['total_timesteps'],\n",
    "        config_path=str(config_path),\n",
    "        resume=True,  # Resume from checkpoint if exists\n",
    "    )\n",
    "    \n",
    "    if model is not None:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úì PPO training completed successfully!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Display training summary\n",
    "        from colab_utils import get_ppo_models_path, get_checkpoints_path  # type: ignore\n",
    "        final_model_path = get_ppo_models_path() / f\"ppo_{config['models']['prediction_model']}_{DATASET_NAME}.zip\"\n",
    "        if final_model_path.exists():\n",
    "            size_mb = final_model_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"\\nFinal model saved: {final_model_path.name} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        checkpoint_path = get_checkpoints_path() / f\"{config['models']['prediction_model']}_{DATASET_NAME}\"\n",
    "        if checkpoint_path.exists():\n",
    "            checkpoints = list(checkpoint_path.glob(\"*.zip\"))\n",
    "            print(f\"Checkpoints available: {len(checkpoints)}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö† PPO training returned None - check for errors above\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during PPO training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Summary and Verification\n",
    "# ==================================\n",
    "\n",
    "print(\"Training Summary and Verification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# List all trained prediction models\n",
    "print(\"\\n1. Trained Prediction Models:\")\n",
    "print(\"-\" * 60)\n",
    "from colab_utils import get_models_path  # type: ignore\n",
    "models_path = get_models_path()\n",
    "\n",
    "model_files = list(models_path.glob(f\"*{DATASET_NAME}*classification.keras\"))\n",
    "if model_files:\n",
    "    total_size = 0\n",
    "    for model_file in sorted(model_files):\n",
    "        size_mb = model_file.stat().st_size / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f\"  ‚úì {model_file.name} ({size_mb:.2f} MB)\")\n",
    "    print(f\"  Total size: {total_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"  ‚ö† No prediction models found\")\n",
    "\n",
    "# List PPO checkpoints and final model\n",
    "print(\"\\n2. PPO Models and Checkpoints:\")\n",
    "print(\"-\" * 60)\n",
    "from colab_utils import get_ppo_models_path, get_checkpoints_path  # type: ignore\n",
    "ppo_models_path = get_ppo_models_path()\n",
    "checkpoints_path = get_checkpoints_path()\n",
    "\n",
    "# Final PPO model\n",
    "ppo_model_pattern = f\"ppo_*{DATASET_NAME}.zip\"\n",
    "ppo_models = list(ppo_models_path.glob(ppo_model_pattern))\n",
    "if ppo_models:\n",
    "    for model_file in sorted(ppo_models):\n",
    "        size_mb = model_file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  ‚úì Final PPO model: {model_file.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_dir = checkpoints_path / f\"ensemble_{DATASET_NAME}\"\n",
    "if not checkpoint_dir.exists():\n",
    "    # Try other possible names\n",
    "    for pattern in [f\"*{DATASET_NAME}*\", f\"*ensemble*\"]:\n",
    "        matches = list(checkpoints_path.glob(pattern))\n",
    "        if matches and matches[0].is_dir():\n",
    "            checkpoint_dir = matches[0]\n",
    "            break\n",
    "\n",
    "if checkpoint_dir.exists() and checkpoint_dir.is_dir():\n",
    "    checkpoints = list(checkpoint_dir.glob(\"*.zip\"))\n",
    "    if checkpoints:\n",
    "        print(f\"  ‚úì Checkpoints found: {len(checkpoints)} files\")\n",
    "        latest = max(checkpoints, key=lambda p: p.stat().st_mtime)\n",
    "        size_mb = latest.stat().st_size / (1024 * 1024)\n",
    "        print(f\"    Latest: {latest.name} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö† Checkpoint directory exists but no .zip files found\")\n",
    "else:\n",
    "    print(f\"  ‚ö† No checkpoint directory found\")\n",
    "\n",
    "# Display training summary\n",
    "print(\"\\n3. Training Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Dataset: {DATASET_NAME}\")\n",
    "print(f\"  Prediction models trained: {len([m for m in ['lstm', 'gru', 'bilstm', 'dlstm'] if (models_path / f'{m}_{DATASET_NAME}_classification.keras').exists()])}/4\")\n",
    "print(f\"  PPO model: {'‚úì Trained' if ppo_models else '‚úó Not found'}\")\n",
    "print(f\"  Prediction horizons: {config['models'].get('prediction_horizons', [1, 2, 3, 5, 10])}\")\n",
    "\n",
    "# Verify all files saved to Google Drive\n",
    "print(\"\\n4. Google Drive Verification:\")\n",
    "print(\"-\" * 60)\n",
    "project_path = Path(PROJECT_PATH)\n",
    "if '/content/drive' in str(project_path):\n",
    "    print(f\"  ‚úì Files saved to Google Drive\")\n",
    "    print(f\"    Path: {project_path}\")\n",
    "else:\n",
    "    print(f\"  ‚ö† Not running on Colab - files saved locally\")\n",
    "\n",
    "# Show model paths for future use\n",
    "print(\"\\n5. Model Paths for Future Use:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Project root: {PROJECT_PATH}\")\n",
    "print(f\"  Prediction models: {models_path}\")\n",
    "print(f\"  PPO models: {ppo_models_path}\")\n",
    "print(f\"  Checkpoints: {checkpoints_path}\")\n",
    "print(f\"  Dataset: {get_datasets_path()}\")\n",
    "\n",
    "# Display next steps\n",
    "print(\"\\n6. Next Steps:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  1. Evaluate Models:\")\n",
    "print(f\"     python 'PPO approach/evaluate_ppo.py' --model ensemble --dataset {DATASET_NAME}\")\n",
    "print(\"  2. Backtest:\")\n",
    "print(\"     Run backtests on historical data to verify profitability\")\n",
    "print(\"  3. Deploy:\")\n",
    "print(\"     Integrate models into live trading bot (separate implementation)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAll models and results saved to: {PROJECT_PATH}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
