[ENVIRONMENT]
# Action space type: discrete or continuous
action_space = discrete
# Transaction cost per trade (0.0025 = 0.25%)
transaction_cost = 0.0025
# Initial capital for simulation
initial_capital = 10000
# Maximum position size (1.0 = 100% of capital)
max_position = 1.0
# Sequence length for prediction models
sequence_length = 60

[PPO]
# Learning rate - will decay linearly during training
learning_rate = 0.0003
learning_rate_final = 0.0001
# Steps per update (OPTIMIZED for RTX 4090 - balance GPU utilization and sample efficiency)
# With 12-16 parallel environments, n_steps=1024 means ~64-85 steps per env before training
# Increased to 1024 for better sample efficiency while maintaining high GPU utilization
# More parallel envs (12-16) means faster data collection, so we can use more steps
# NOTE: With vectorized envs, this is total steps across all parallel envs
n_steps = 1024
# Batch size (OPTIMIZED for RTX 4090 with 24GB VRAM - target: 90-95% utilization)
# RTX 4090 has 24GB VRAM, but we need to stay under 24GB to avoid OOM
# Set to 2048 for safe operation (~16-18GB VRAM usage) - can increase if stable
# Combined with larger network [1536, 768], this should reach 85-90% GPU usage
batch_size = 2048
# Training epochs per update (more epochs = more GPU computation per update)
# Increased to 20 for MAXIMUM GPU utilization (target: 90-95%)
# More epochs = more GPU computation per update cycle
# With larger batch size (2560), more epochs will fully utilize RTX 4090
n_epochs = 20 
# Discount factor
gamma = 0.99
# GAE lambda
gae_lambda = 0.95
# Clipping range
clip_range = 0.2
# Entropy coefficient (encourages exploration)
# Increased from 0.01 to 0.05 to prevent agent from only holding (action 0)
# Higher entropy encourages exploration of all actions, not just the "safe" hold action
# NOTE: Will decay linearly from 0.05 to 0.01 during training (implemented in code)
ent_coef = 0.05
ent_coef_final = 0.01
# Value function coefficient
vf_coef = 0.5
# Max gradient norm
max_grad_norm = 0.5

[REWARD]
# Scale factor for profit (increased from 50 to 100 to make profits more attractive)
# Higher profit_scale encourages the agent to take trades that generate profit
# This helps prevent the agent from learning to only hold (action 0)
profit_scale = 100
# Transaction cost penalty scale
cost_scale = 1.0
# Drawdown penalty weight (reduced from 0.1 to be less aggressive)
drawdown_penalty = 0.05
# Max drawdown threshold before increased penalty
max_drawdown_threshold = 0.1
# Sharpe ratio bonus weight (increased from 0.5 to encourage better risk-adjusted returns)
sharpe_bonus = 1.5
# Minimum periods for Sharpe calculation
min_periods_for_sharpe = 10
# Enable hold penalty (discourage excessive holding) - now enabled
enable_hold_penalty = true
# Hold penalty weight (if enabled) - increased from 0.005 to 0.01
# Higher hold penalty discourages excessive holding and encourages trading activity
hold_penalty = 0.01
# Max hold periods before penalty kicks in
max_hold_periods = 100
# Invalid action penalty
invalid_action_penalty = -1.0
# Clip reward values
clip_reward = true
reward_clip_value = 10.0
# Profit threshold bonus (reward for achieving positive returns)
enable_profit_threshold_bonus = true
profit_threshold = 0.05
profit_threshold_bonus = 5.0

[TRAINING]
# Total training timesteps - INCREASED for better learning
# Previous: 150,000 timesteps = ~10 minutes
# Increased to 300,000 timesteps = ~20-25 minutes (with optimizations)
# This gives agent more time to explore and learn complex strategies
# For even better results, consider 500,000 (35-40 minutes)
total_timesteps = 300000
# Checkpoint frequency (for Colab timeout recovery)
checkpoint_freq = 50000
# Evaluation frequency
eval_freq = 10000
# Number of evaluation episodes - INCREASED for more reliable estimates
n_eval_episodes = 10
# Train/test split - updated to 0.6 for three-way split (60/20/20)
# This prevents data leakage: prediction models train on 60%, PPO trains on 60%,
# validates on 20%, and tests on 20% (models never see val/test data)
train_test_split = 0.6
# Validation split - 20% of data for PPO validation during training
# Validation set is used for hyperparameter tuning and early stopping
# Prediction models never see this data, ensuring proper generalization
validation_split = 0.2
# Maximum steps per episode - INCREASED for longer trading scenarios
# Longer episodes = more trading opportunities and better strategy evaluation
max_episode_steps = 2000
# Device: auto, cpu, cuda
# Set to 'cuda' to explicitly use GPU (better utilization)
device = cuda

[MODELS]
# Prediction model to use: lstm, gru, bilstm, dlstm, ensemble
# Using ensemble with top 2 models (DLSTM + BiLSTM) for best accuracy
# Expected accuracy: 72-75% (vs 70.93% single DLSTM)
# Ensemble is slightly slower but significantly more accurate
prediction_model = ensemble
# Dataset name (without path)
dataset = ETH-EUR_1H_20240101-20251231
# Models to include in ensemble (comma-separated)
# Using top 2 models from analysis: DLSTM (70.93%) + BiLSTM (63.38%)
# This combination provides best accuracy while maintaining reasonable speed
ensemble_models = dlstm,bilstm
# Prediction horizons for multi-step ahead predictions (comma-separated)
# Reduced from 1,2,3,5,10 to 1,2,3 for faster environment steps
# Removed medium-term horizons (5,10) to reduce observation space and computation
# Format: 1,2,3 (short-term only - faster training)
prediction_horizons = 1,2,3


