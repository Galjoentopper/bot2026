[ENVIRONMENT]
# Action space type: discrete or continuous
action_space = discrete
# Transaction cost per trade (0.0025 = 0.25%)
transaction_cost = 0.0025
# Initial capital for simulation
initial_capital = 10000
# Maximum position size (1.0 = 100% of capital)
max_position = 1.0
# Sequence length for prediction models
sequence_length = 60

[PPO]
# Learning rate - will decay linearly during training
# Less aggressive decay to maintain learning capacity longer
learning_rate = 0.0003
learning_rate_final = 0.00015
# Steps per update - REDUCED for more frequent updates
# Smaller rollouts allow more frequent policy updates, helping exploration
# NOTE: With vectorized envs, this is total steps across all parallel envs
n_steps = 512
# Batch size - REDUCED for more frequent updates
# Smaller batches = more frequent gradient updates = better learning signal
batch_size = 256
# Training epochs per update - REDUCED to prevent overfitting
# Fewer epochs per update prevents overfitting to recent experiences
n_epochs = 10 
# Discount factor - INCREASED for faster credit assignment in trading
# Higher gamma helps agent learn faster in trading environment
gamma = 0.995
# GAE lambda - INCREASED for less variance in advantage estimates
# Higher lambda reduces variance and improves stability
gae_lambda = 0.98
# Clipping range - INCREASED for larger policy updates initially
# Wider clipping allows more exploration early in training
# Will decay to clip_range_final during training
clip_range = 0.3
clip_range_final = 0.2
# Clip range will decay to 0.2 during training (if clip_range_final is set)
# Entropy coefficient (encourages exploration)
# MUCH HIGHER initial entropy to prevent policy collapse
# Agent was collapsing to single action, so we need aggressive exploration
# Higher entropy encourages exploration of all actions, not just the "safe" actions
# NOTE: Three-phase decay schedule (implemented in code):
#   Phase 1 (0-70% training): 0.3 → 0.15 (slow decay, maintain high exploration)
#   Phase 2 (70-90% training): 0.15 → 0.08 (moderate decay)
#   Phase 3 (90-100% training): 0.08 → 0.05 (final decay)
# This keeps high exploration for 70% of training, preventing early collapse
ent_coef = 0.3
ent_coef_final = 0.05
# Value function coefficient - INCREASED for better value estimation
# Higher vf_coef improves value function learning and stability
vf_coef = 0.6
# Max gradient norm - INCREASED to allow larger gradient updates
# Higher max_grad_norm allows more aggressive learning while still preventing explosions
max_grad_norm = 0.8

[REWARD]
# Scale factor for profit (increased from 250 to 500 to make profits more attractive)
# Higher profit_scale encourages the agent to take trades that generate profit
# This helps prevent the agent from learning to only hold (action 0)
# Increased to 500 to ensure profitable trades always yield positive rewards
profit_scale = 500
# Transaction cost penalty scale
cost_scale = 1.0
# Drawdown penalty weight (reduced from 0.05 to 0.02 to be less aggressive)
drawdown_penalty = 0.02
# Max drawdown threshold before increased penalty
max_drawdown_threshold = 0.1
# Sharpe ratio bonus weight (increased from 0.5 to encourage better risk-adjusted returns)
sharpe_bonus = 1.5
# Minimum periods for Sharpe calculation
min_periods_for_sharpe = 10
# Enable hold penalty (discourage excessive holding) - now enabled
enable_hold_penalty = true
# Hold penalty weight (if enabled) - SIGNIFICANTLY INCREASED to prevent policy collapse
# Higher hold penalty discourages excessive holding and encourages trading activity
hold_penalty = 0.05
# Max hold periods before penalty kicks in - REDUCED to trigger penalty sooner
max_hold_periods = 50
# Invalid action penalty
invalid_action_penalty = -1.0
# Clip reward values (increased from 20.0 to 30.0 to allow larger positive rewards)
clip_reward = true
reward_clip_value = 30.0
# Minimum reward floor (prevent extreme negatives)
min_reward_floor = -0.1
# Profit threshold bonus (reward for achieving positive returns)
enable_profit_threshold_bonus = true
profit_threshold = 0.05
profit_threshold_bonus = 10.0
# Inaction penalty (penalize being flat without opening position)
# CRITICAL: This prevents agent from learning to do nothing
# Agent was collapsing to actions that do nothing when flat
# SIGNIFICANTLY INCREASED to strongly discourage inaction
enable_inaction_penalty = true
inaction_penalty = 0.1
# Action-specific bonuses (to encourage buying actions)
# Buy action bonus: SIGNIFICANTLY INCREASED to strongly encourage trading
# Increased to 5.0 to more strongly encourage buying actions and prevent policy collapse
buy_action_bonus = 5.0
# Sell action bonus: INCREASED to encourage selling/closing actions
sell_action_bonus = 1.0
# Transaction cost handling for opening positions
# Only apply 20% of transaction cost penalty when opening positions (reduced from 30%)
# Full cost penalty is applied when closing positions
# This addresses credit assignment problem: opening positions no longer get immediate full penalty
# Further reduced to encourage more trading activity
open_position_cost_ratio = 0.2

[TRAINING]
# Total training timesteps - SIGNIFICANTLY INCREASED for better learning
# Previous: 300,000 timesteps was insufficient - agent collapsed to degenerate policy
# Increased to 1,000,000 timesteps to give agent enough time to explore and learn
# With reduced parallel envs (4 instead of 16), each env sees more diverse experiences
total_timesteps = 1000000
# Checkpoint frequency (for Colab timeout recovery)
checkpoint_freq = 50000
# Evaluation frequency - MORE FREQUENT to catch issues earlier
# More frequent evaluation helps detect policy collapse and training issues sooner
eval_freq = 5000
# Number of evaluation episodes - INCREASED for more reliable estimates
n_eval_episodes = 10
# Train/test split - updated to 0.6 for three-way split (60/20/20)
# This prevents data leakage: prediction models train on 60%, PPO trains on 60%,
# validates on 20%, and tests on 20% (models never see val/test data)
train_test_split = 0.6
# Validation split - 20% of data for PPO validation during training
# Validation set is used for hyperparameter tuning and early stopping
# Prediction models never see this data, ensuring proper generalization
validation_split = 0.2
# Maximum steps per episode - INCREASED for longer trading scenarios
# Longer episodes = more trading opportunities and better strategy evaluation
max_episode_steps = 2000
# Device: auto, cpu, cuda
# Set to 'cuda' to explicitly use GPU (better utilization)
device = cuda

[MODELS]
# Prediction model to use: lstm, gru, bilstm, dlstm, ensemble
# Using ensemble with top 2 models (DLSTM + BiLSTM) for best accuracy
# Expected accuracy: 72-75% (vs 70.93% single DLSTM)
# Ensemble is slightly slower but significantly more accurate
prediction_model = ensemble
# Dataset name (without path)
dataset = ETH-EUR_1H_20240101-20251231
# Models to include in ensemble (comma-separated)
# Using top 2 models from analysis: DLSTM (70.93%) + BiLSTM (63.38%)
# This combination provides best accuracy while maintaining reasonable speed
ensemble_models = dlstm,bilstm
# Prediction horizons for multi-step ahead predictions (comma-separated)
# Reduced from 1,2,3,5,10 to 1,2,3 for faster environment steps
# Removed medium-term horizons (5,10) to reduce observation space and computation
# Format: 1,2,3 (short-term only - faster training)
prediction_horizons = 1,2,3


