[ENVIRONMENT]
# Action space type: discrete or continuous
action_space = discrete
# Transaction cost per trade (0.0025 = 0.25%)
transaction_cost = 0.0025
# Initial capital for simulation
initial_capital = 10000
# Maximum position size (1.0 = 100% of capital)
max_position = 1.0
# Sequence length for prediction models
sequence_length = 60

[PPO]
# Learning rate
learning_rate = 0.0003
# Steps per update (OPTIMIZED for RTX 4090 - balance GPU utilization and sample efficiency)
# With 12-16 parallel environments, n_steps=1024 means ~64-85 steps per env before training
# Increased to 1024 for better sample efficiency while maintaining high GPU utilization
# More parallel envs (12-16) means faster data collection, so we can use more steps
# NOTE: With vectorized envs, this is total steps across all parallel envs
n_steps = 1024
# Batch size (OPTIMIZED for RTX 4090 with 24GB VRAM - target: 90-95% utilization)
# RTX 4090 has 24GB VRAM, so we can use much larger batches
# Increased to 2560 for maximum GPU utilization (~18-20GB VRAM usage)
# Combined with larger network [1536, 768], this should reach 90-95% GPU usage
batch_size = 2560
# Training epochs per update (more epochs = more GPU computation per update)
# Increased to 20 for MAXIMUM GPU utilization (target: 90-95%)
# More epochs = more GPU computation per update cycle
# With larger batch size (2560), more epochs will fully utilize RTX 4090
n_epochs = 20 
# Discount factor
gamma = 0.99
# GAE lambda
gae_lambda = 0.95
# Clipping range
clip_range = 0.2
# Entropy coefficient (encourages exploration)
ent_coef = 0.01
# Value function coefficient
vf_coef = 0.5
# Max gradient norm
max_grad_norm = 0.5

[REWARD]
# Scale factor for profit
profit_scale = 100
# Transaction cost penalty scale
cost_scale = 1.0
# Drawdown penalty weight
drawdown_penalty = 0.1
# Max drawdown threshold before increased penalty
max_drawdown_threshold = 0.1
# Sharpe ratio bonus weight
sharpe_bonus = 0.5
# Minimum periods for Sharpe calculation
min_periods_for_sharpe = 10
# Enable hold penalty (discourage excessive holding)
enable_hold_penalty = false
# Hold penalty weight (if enabled)
hold_penalty = 0.01
# Max hold periods before penalty kicks in
max_hold_periods = 100
# Invalid action penalty
invalid_action_penalty = -1.0
# Clip reward values
clip_reward = true
reward_clip_value = 10.0

[TRAINING]
# Total training timesteps - REDUCED for 4-hour training target
# Original: 1,000,000 timesteps = 80 hours
# Reduced to 150,000 timesteps = ~4 hours (with optimizations)
# This is 15% of original - model will learn basic strategies
# For better results, consider 200,000-300,000 (6-8 hours)
total_timesteps = 150000
# Checkpoint frequency (for Colab timeout recovery)
checkpoint_freq = 25000
# Evaluation frequency
eval_freq = 5000
# Number of evaluation episodes
n_eval_episodes = 5
# Train/test split
train_test_split = 0.8
# Maximum steps per episode (None = use full data)
max_episode_steps = 1000
# Device: auto, cpu, cuda
# Set to 'cuda' to explicitly use GPU (better utilization)
device = cuda

[MODELS]
# Prediction model to use: lstm, gru, bilstm, dlstm, ensemble
# Changed from ensemble to dlstm for 4x faster environment steps
# Ensemble (4 models) is slower but more accurate
# Single model (dlstm) is faster and still effective for PPO training
prediction_model = dlstm
# Dataset name (without path)
dataset = ETH-EUR_1H_20240101-20251231
# Models to include in ensemble (comma-separated) - not used when prediction_model != ensemble
ensemble_models = lstm,gru,bilstm,dlstm
# Prediction horizons for multi-step ahead predictions (comma-separated)
# Reduced from 1,2,3,5,10 to 1,2,3 for faster environment steps
# Removed medium-term horizons (5,10) to reduce observation space and computation
# Format: 1,2,3 (short-term only - faster training)
prediction_horizons = 1,2,3


