[ENVIRONMENT]
# Action space type: discrete or continuous
action_space = discrete
# Transaction cost per trade (0.0025 = 0.25%)
transaction_cost = 0.0025
# Initial capital for simulation
initial_capital = 10000
# Maximum position size (1.0 = 100% of capital)
max_position = 1.0
# Sequence length for prediction models
sequence_length = 60

[PPO]
# Learning rate - will decay linearly during training
learning_rate = 0.0003
learning_rate_final = 0.0001
# Steps per update - REDUCED for more frequent updates
# Smaller rollouts allow more frequent policy updates, helping exploration
# NOTE: With vectorized envs, this is total steps across all parallel envs
n_steps = 512
# Batch size - REDUCED for more frequent updates
# Smaller batches = more frequent gradient updates = better learning signal
batch_size = 256
# Training epochs per update - REDUCED to prevent overfitting
# Fewer epochs per update prevents overfitting to recent experiences
n_epochs = 10 
# Discount factor
gamma = 0.99
# GAE lambda
gae_lambda = 0.95
# Clipping range
clip_range = 0.2
# Entropy coefficient (encourages exploration)
# MUCH HIGHER initial entropy to prevent policy collapse
# Agent was collapsing to single action (Action 5), so we need aggressive exploration
# Higher entropy encourages exploration of all actions, not just the "safe" actions
# NOTE: Will decay linearly from 0.15 to 0.02 during training (implemented in code)
ent_coef = 0.15
ent_coef_final = 0.02
# Value function coefficient
vf_coef = 0.5
# Max gradient norm
max_grad_norm = 0.5

[REWARD]
# Scale factor for profit (increased from 50 to 100 to make profits more attractive)
# Higher profit_scale encourages the agent to take trades that generate profit
# This helps prevent the agent from learning to only hold (action 0)
profit_scale = 100
# Transaction cost penalty scale
cost_scale = 1.0
# Drawdown penalty weight (reduced from 0.1 to be less aggressive)
drawdown_penalty = 0.05
# Max drawdown threshold before increased penalty
max_drawdown_threshold = 0.1
# Sharpe ratio bonus weight (increased from 0.5 to encourage better risk-adjusted returns)
sharpe_bonus = 1.5
# Minimum periods for Sharpe calculation
min_periods_for_sharpe = 10
# Enable hold penalty (discourage excessive holding) - now enabled
enable_hold_penalty = true
# Hold penalty weight (if enabled) - increased from 0.005 to 0.01
# Higher hold penalty discourages excessive holding and encourages trading activity
hold_penalty = 0.01
# Max hold periods before penalty kicks in
max_hold_periods = 100
# Invalid action penalty
invalid_action_penalty = -1.0
# Clip reward values
clip_reward = true
reward_clip_value = 10.0
# Profit threshold bonus (reward for achieving positive returns)
enable_profit_threshold_bonus = true
profit_threshold = 0.05
profit_threshold_bonus = 5.0
# Inaction penalty (penalize being flat without opening position)
# CRITICAL: This prevents agent from learning to do nothing
# Agent was collapsing to actions that do nothing when flat (e.g., Action 5: Sell Medium)
enable_inaction_penalty = true
inaction_penalty = 0.03

[TRAINING]
# Total training timesteps - SIGNIFICANTLY INCREASED for better learning
# Previous: 300,000 timesteps was insufficient - agent collapsed to degenerate policy
# Increased to 1,000,000 timesteps to give agent enough time to explore and learn
# With reduced parallel envs (4 instead of 16), each env sees more diverse experiences
total_timesteps = 1000000
# Checkpoint frequency (for Colab timeout recovery)
checkpoint_freq = 50000
# Evaluation frequency
eval_freq = 10000
# Number of evaluation episodes - INCREASED for more reliable estimates
n_eval_episodes = 10
# Train/test split - updated to 0.6 for three-way split (60/20/20)
# This prevents data leakage: prediction models train on 60%, PPO trains on 60%,
# validates on 20%, and tests on 20% (models never see val/test data)
train_test_split = 0.6
# Validation split - 20% of data for PPO validation during training
# Validation set is used for hyperparameter tuning and early stopping
# Prediction models never see this data, ensuring proper generalization
validation_split = 0.2
# Maximum steps per episode - INCREASED for longer trading scenarios
# Longer episodes = more trading opportunities and better strategy evaluation
max_episode_steps = 2000
# Device: auto, cpu, cuda
# Set to 'cuda' to explicitly use GPU (better utilization)
device = cuda

[MODELS]
# Prediction model to use: lstm, gru, bilstm, dlstm, ensemble
# Using ensemble with top 2 models (DLSTM + BiLSTM) for best accuracy
# Expected accuracy: 72-75% (vs 70.93% single DLSTM)
# Ensemble is slightly slower but significantly more accurate
prediction_model = ensemble
# Dataset name (without path)
dataset = ETH-EUR_1H_20240101-20251231
# Models to include in ensemble (comma-separated)
# Using top 2 models from analysis: DLSTM (70.93%) + BiLSTM (63.38%)
# This combination provides best accuracy while maintaining reasonable speed
ensemble_models = dlstm,bilstm
# Prediction horizons for multi-step ahead predictions (comma-separated)
# Reduced from 1,2,3,5,10 to 1,2,3 for faster environment steps
# Removed medium-term horizons (5,10) to reduce observation space and computation
# Format: 1,2,3 (short-term only - faster training)
prediction_horizons = 1,2,3


